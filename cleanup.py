#!/usr/bin/env python3
"""
FF-KB-Robot æ•°æ®æ¸…ç†è„šæœ¬ - å…¨é¢æ¸…é™¤æ‰€æœ‰æ•°æ®å’Œç¼“å­˜

åŠŸèƒ½ï¼š
1. æ¸…é™¤ SQLite æ•°æ®åº“ä¸­çš„æ‰€æœ‰çŸ¥è¯†åº“ã€æ–‡æ¡£ã€åˆ†å—æ•°æ®
2. æ¸…é™¤å‘é‡æ•°æ®åº“ï¼ˆHNSW ç´¢å¼•å’Œå…ƒæ•°æ®ï¼‰
3. æ¸…é™¤è¿è¡Œæ—¶ç¼“å­˜ï¼ˆEmbeddingã€æŸ¥è¯¢ç»“æœã€åˆ†ç±»å™¨ç¼“å­˜ï¼‰
4. æ¸…é™¤ä¸´æ—¶ä¸Šä¼ æ–‡ä»¶å’Œå¤„ç†åçš„åˆ†å—
5. æ¸…é™¤æ—¥å¿—æ–‡ä»¶
6. æ¸…é™¤ Python ç¼–è¯‘ç¼“å­˜ï¼ˆ__pycache__ã€.pycï¼‰
7. æ¸…é™¤å…¶ä»–å¼€å‘ç¼“å­˜ï¼ˆ.pytest_cacheã€.mypy_cache ç­‰ï¼‰
8. æ”¯æŒæ•°æ®åº“è‡ªåŠ¨å¤‡ä»½

ä½¿ç”¨ç¤ºä¾‹ï¼š
    python cleanup.py                    # äº¤äº’æ¨¡å¼ï¼ˆæ¨èï¼‰
    python cleanup.py --all --backup     # å¤‡ä»½å¹¶æ¸…é™¤æ‰€æœ‰ï¼ˆæ— éœ€ç¡®è®¤ï¼‰
    python cleanup.py --all --no-backup  # æ¸…é™¤æ‰€æœ‰ï¼ˆå±é™©ï¼ï¼‰
    python cleanup.py --only-cache       # ä»…æ¸…é™¤ç¼“å­˜ï¼ˆä¿ç•™æ•°æ®ï¼‰

æ³¨æ„ï¼šæ­¤æ“ä½œä¸å¯é€†ï¼å¼ºçƒˆå»ºè®®å…ˆå¤‡ä»½ï¼
"""

import os
import sys
import shutil
import sqlite3
import argparse
import logging
from pathlib import Path
from datetime import datetime
from typing import Optional, List, Tuple

# ==================== é…ç½® ====================

PROJECT_ROOT = Path(__file__).parent
DB_DIR = PROJECT_ROOT / "db"
DATA_DIR = PROJECT_ROOT / "data"
LOGS_DIR = PROJECT_ROOT / "logs"

# æ•°æ®ç›®å½•
VECTOR_STORE_DIR = DB_DIR / "vector_store"
SQL_DB_DIR = DB_DIR / "sql_db"
TEMP_UPLOADS_DIR = DATA_DIR / "temp_uploads"
PROCESSED_CHUNKS_DIR = DATA_DIR / "processed_chunks"

# æ•°æ®åº“æ–‡ä»¶
SQL_DB_FILE = SQL_DB_DIR / "kbrobot.db"

# ç¼“å­˜ç›®å½•æ¨¡å¼
PYCACHE_PATTERN = "**/__pycache__"
PYTEST_CACHE_DIR = PROJECT_ROOT / ".pytest_cache"
MYPY_CACHE_DIR = PROJECT_ROOT / ".mypy_cache"
RUFF_CACHE_DIR = PROJECT_ROOT / ".ruff_cache"

# ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
LOGS_DIR.mkdir(parents=True, exist_ok=True)
LOG_FILE = LOGS_DIR / "cleanup.log"

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


# ==================== ç»Ÿè®¡ç±» ====================

class CleanupStats:
    """æ¸…ç†ç»Ÿè®¡"""
    def __init__(self):
        self.files_deleted = 0
        self.dirs_deleted = 0
        self.bytes_freed = 0
        self.errors = []

    def add_file(self, size: int = 0):
        self.files_deleted += 1
        self.bytes_freed += size

    def add_dir(self):
        self.dirs_deleted += 1

    def add_error(self, msg: str):
        self.errors.append(msg)

    def format_size(self) -> str:
        """æ ¼å¼åŒ–å­—èŠ‚å¤§å°"""
        size = self.bytes_freed
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size < 1024:
                return f"{size:.2f} {unit}"
            size /= 1024
        return f"{size:.2f} TB"

    def summary(self) -> str:
        return f"{self.files_deleted} æ–‡ä»¶, {self.dirs_deleted} ç›®å½•, é‡Šæ”¾ {self.format_size()}"


# ==================== æ¸…ç†å‡½æ•° ====================

def show_database_stats(db_path: Path) -> Tuple[int, int, int]:
    """æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
    if not db_path.exists():
        return 0, 0, 0

    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        cursor.execute("SELECT COUNT(*) FROM knowledge_bases")
        kb_count = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM documents")
        doc_count = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM text_chunks")
        chunk_count = cursor.fetchone()[0]

        conn.close()
        return kb_count, doc_count, chunk_count

    except sqlite3.Error:
        return 0, 0, 0


def backup_database(db_path: Path) -> Optional[Path]:
    """å¤‡ä»½æ•°æ®åº“æ–‡ä»¶"""
    if not db_path.exists():
        logger.warning(f"æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡å¤‡ä»½: {db_path}")
        return None

    backup_dir = PROJECT_ROOT / "db_backups"
    backup_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_file = backup_dir / f"kbrobot_{timestamp}.db"

    try:
        shutil.copy2(db_path, backup_file)
        logger.info(f"âœ“ æ•°æ®åº“å·²å¤‡ä»½: {backup_file}")
        return backup_file
    except Exception as e:
        logger.error(f"âœ— æ•°æ®åº“å¤‡ä»½å¤±è´¥: {e}")
        return None


def clear_database(db_path: Path, stats: CleanupStats) -> bool:
    """æ¸…é™¤ SQLite æ•°æ®åº“ä¸­çš„æ‰€æœ‰çŸ¥è¯†åº“ã€æ–‡æ¡£ã€åˆ†å—æ•°æ®"""
    if not db_path.exists():
        logger.warning(f"æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡æ¸…ç†: {db_path}")
        return True

    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        # è·å–æ¸…ç†å‰çš„æ•°æ®ç»Ÿè®¡
        cursor.execute("SELECT COUNT(*) FROM knowledge_bases")
        kb_count = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM documents")
        doc_count = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM text_chunks")
        chunk_count = cursor.fetchone()[0]

        # æ¸…é™¤æ•°æ®ï¼ˆä¿ç•™è¡¨ç»“æ„ï¼‰
        cursor.execute("DELETE FROM text_chunks")
        cursor.execute("DELETE FROM documents")
        cursor.execute("DELETE FROM knowledge_bases")

        conn.commit()
        conn.close()

        logger.info(f"âœ“ å·²æ¸…é™¤æ•°æ®åº“: {kb_count} ä¸ªçŸ¥è¯†åº“ã€{doc_count} ä¸ªæ–‡æ¡£ã€{chunk_count} ä¸ªåˆ†å—")
        return True

    except sqlite3.Error as e:
        logger.error(f"âœ— æ•°æ®åº“æ¸…ç†å¤±è´¥: {e}")
        stats.add_error(f"æ•°æ®åº“æ¸…ç†å¤±è´¥: {e}")
        return False


def clear_directory(dir_path: Path, stats: CleanupStats, description: str) -> bool:
    """æ¸…é™¤ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶å’Œå­ç›®å½•"""
    if not dir_path.exists():
        logger.warning(f"{description}ç›®å½•ä¸å­˜åœ¨: {dir_path}")
        return True

    try:
        count = 0
        for item in dir_path.iterdir():
            try:
                if item.is_file():
                    size = item.stat().st_size
                    item.unlink()
                    stats.add_file(size)
                    count += 1
                elif item.is_dir():
                    shutil.rmtree(item)
                    stats.add_dir()
                    count += 1
            except Exception as e:
                stats.add_error(f"åˆ é™¤å¤±è´¥ {item}: {e}")

        logger.info(f"âœ“ å·²æ¸…é™¤{description}: {count} ä¸ªæ–‡ä»¶/ç›®å½•")
        return True

    except Exception as e:
        logger.error(f"âœ— {description}æ¸…ç†å¤±è´¥: {e}")
        stats.add_error(f"{description}æ¸…ç†å¤±è´¥: {e}")
        return False


def clear_pycache(stats: CleanupStats) -> bool:
    """æ¸…é™¤æ‰€æœ‰ __pycache__ ç›®å½•"""
    try:
        count = 0
        for pycache_dir in PROJECT_ROOT.glob(PYCACHE_PATTERN):
            try:
                shutil.rmtree(pycache_dir)
                stats.add_dir()
                count += 1
            except Exception as e:
                stats.add_error(f"åˆ é™¤å¤±è´¥ {pycache_dir}: {e}")

        # æ¸…é™¤ .pyc æ–‡ä»¶
        for pyc_file in PROJECT_ROOT.glob("**/*.pyc"):
            try:
                size = pyc_file.stat().st_size
                pyc_file.unlink()
                stats.add_file(size)
                count += 1
            except Exception as e:
                stats.add_error(f"åˆ é™¤å¤±è´¥ {pyc_file}: {e}")

        # æ¸…é™¤ .pyo æ–‡ä»¶
        for pyo_file in PROJECT_ROOT.glob("**/*.pyo"):
            try:
                size = pyo_file.stat().st_size
                pyo_file.unlink()
                stats.add_file(size)
                count += 1
            except Exception as e:
                stats.add_error(f"åˆ é™¤å¤±è´¥ {pyo_file}: {e}")

        logger.info(f"âœ“ å·²æ¸…é™¤ Python ç¼–è¯‘ç¼“å­˜: {count} ä¸ªæ–‡ä»¶/ç›®å½•")
        return True

    except Exception as e:
        logger.error(f"âœ— Python ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")
        stats.add_error(f"Python ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")
        return False


def clear_dev_caches(stats: CleanupStats) -> bool:
    """æ¸…é™¤å¼€å‘å·¥å…·ç¼“å­˜ï¼ˆpytest, mypy, ruff ç­‰ï¼‰"""
    dev_cache_dirs = [
        (PYTEST_CACHE_DIR, ".pytest_cache"),
        (MYPY_CACHE_DIR, ".mypy_cache"),
        (RUFF_CACHE_DIR, ".ruff_cache"),
    ]

    count = 0
    for cache_dir, name in dev_cache_dirs:
        if cache_dir.exists():
            try:
                shutil.rmtree(cache_dir)
                stats.add_dir()
                count += 1
            except Exception as e:
                stats.add_error(f"åˆ é™¤å¤±è´¥ {name}: {e}")

    if count > 0:
        logger.info(f"âœ“ å·²æ¸…é™¤å¼€å‘å·¥å…·ç¼“å­˜: {count} ä¸ªç›®å½•")
    else:
        logger.info("âœ“ æ— å¼€å‘å·¥å…·ç¼“å­˜éœ€è¦æ¸…ç†")

    return True


def clear_runtime_cache() -> bool:
    """æ¸…é™¤è¿è¡Œæ—¶å†…å­˜ç¼“å­˜ï¼ˆCacheManagerï¼‰"""
    try:
        # å°è¯•å¯¼å…¥å¹¶æ¸…ç†ç¼“å­˜ç®¡ç†å™¨
        from utils.cache_manager import get_cache_manager, _cache_manager_instance

        # å¦‚æœç¼“å­˜ç®¡ç†å™¨å·²åˆå§‹åŒ–ï¼Œæ¸…ç©ºå®ƒ
        if _cache_manager_instance is not None:
            _cache_manager_instance.clear_all()
            logger.info("âœ“ å·²æ¸…é™¤è¿è¡Œæ—¶ç¼“å­˜ï¼ˆCacheManagerï¼‰")
        else:
            logger.info("âœ“ è¿è¡Œæ—¶ç¼“å­˜æœªåˆå§‹åŒ–ï¼Œæ— éœ€æ¸…ç†")

        return True

    except ImportError:
        logger.warning("âš  æ— æ³•å¯¼å…¥ç¼“å­˜ç®¡ç†å™¨ï¼Œè·³è¿‡è¿è¡Œæ—¶ç¼“å­˜æ¸…ç†")
        return True
    except Exception as e:
        logger.warning(f"âš  è¿è¡Œæ—¶ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")
        return True  # ä¸ç®—å¤±è´¥ï¼Œå› ä¸ºæ˜¯å¯é€‰æ¸…ç†


def clear_logs(logs_dir: Path, stats: CleanupStats, keep_cleanup_log: bool = True) -> bool:
    """æ¸…é™¤æ—¥å¿—æ–‡ä»¶"""
    if not logs_dir.exists():
        logger.warning(f"æ—¥å¿—ç›®å½•ä¸å­˜åœ¨: {logs_dir}")
        return True

    try:
        count = 0
        for log_file in logs_dir.glob("*.log"):
            if keep_cleanup_log and log_file.name == "cleanup.log":
                continue
            try:
                size = log_file.stat().st_size
                log_file.unlink()
                stats.add_file(size)
                count += 1
            except Exception as e:
                stats.add_error(f"åˆ é™¤å¤±è´¥ {log_file}: {e}")

        logger.info(f"âœ“ å·²æ¸…é™¤æ—¥å¿—æ–‡ä»¶: {count} ä¸ª")
        return True

    except Exception as e:
        logger.error(f"âœ— æ—¥å¿—æ–‡ä»¶æ¸…ç†å¤±è´¥: {e}")
        stats.add_error(f"æ—¥å¿—æ–‡ä»¶æ¸…ç†å¤±è´¥: {e}")
        return False


def confirm(prompt: str, default: bool = False) -> bool:
    """ç¡®è®¤ç”¨æˆ·é€‰æ‹©"""
    default_text = "yes" if default else "no"
    response = input(f"{prompt} (yes/no) [{default_text}]: ").strip().lower()

    if response in ['yes', 'y']:
        return True
    elif response in ['no', 'n']:
        return False
    else:
        return default


# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="FF-KB-Robot æ•°æ®æ¸…ç†å·¥å…· ğŸ±",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python cleanup.py              # äº¤äº’æ¨¡å¼ï¼ˆæ¨èï¼‰
  python cleanup.py --all        # æ¸…é™¤æ‰€æœ‰æ•°æ®å’Œç¼“å­˜
  python cleanup.py --only-cache # ä»…æ¸…é™¤ç¼“å­˜ï¼ˆä¿ç•™æ•°æ®ï¼‰
  python cleanup.py --backup     # æ¸…ç†å‰è‡ªåŠ¨å¤‡ä»½
"""
    )

    parser.add_argument("--all", action="store_true", help="ä¸€æ¬¡æ¸…é™¤æ‰€æœ‰æ•°æ®ï¼ˆæ— éœ€é€é¡¹ç¡®è®¤ï¼‰")
    parser.add_argument("--only-cache", action="store_true", help="ä»…æ¸…é™¤ç¼“å­˜ï¼ˆä¿ç•™æ•°æ®åº“å’Œæ–‡æ¡£ï¼‰")
    parser.add_argument("--backup", action="store_true", help="æ¸…ç†å‰è‡ªåŠ¨å¤‡ä»½æ•°æ®åº“")
    parser.add_argument("--no-backup", action="store_true", help="ä¸å¤‡ä»½æ•°æ®åº“ï¼ˆå±é™©ï¼ï¼‰")

    args = parser.parse_args()

    # åˆå§‹åŒ–ç»Ÿè®¡
    stats = CleanupStats()

    # æ‰“å°æ¬¢è¿ä¿¡æ¯
    print("\n" + "=" * 70)
    print("  FF-KB-Robot æ•°æ®æ¸…ç†å·¥å…· ğŸ±")
    print("=" * 70)
    print(f"é¡¹ç›®è·¯å¾„: {PROJECT_ROOT}")
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()

    # æ—¥å¿—è®°å½•
    logger.info("=" * 70)
    logger.info("FF-KB-Robot æ•°æ®æ¸…ç†å¼€å§‹")
    logger.info("=" * 70)

    # æ˜¾ç¤ºå½“å‰æ•°æ®åº“ç»Ÿè®¡
    if SQL_DB_FILE.exists():
        kb_count, doc_count, chunk_count = show_database_stats(SQL_DB_FILE)
        if kb_count > 0 or doc_count > 0 or chunk_count > 0:
            print("ğŸ“Š å½“å‰æ•°æ®åº“çŠ¶æ€:")
            print(f"  â€¢ çŸ¥è¯†åº“: {kb_count} ä¸ª")
            print(f"  â€¢ æ–‡æ¡£: {doc_count} ä¸ª")
            print(f"  â€¢ åˆ†å—: {chunk_count} ä¸ª")
            print()

    # æ˜¾ç¤ºæ¸…ç†é¡¹ç›®
    if args.only_cache:
        print("ğŸ“‹ ä»…æ¸…ç†ç¼“å­˜æ¨¡å¼:")
        print("  1. Python ç¼–è¯‘ç¼“å­˜ (__pycache__, .pyc)")
        print("  2. å¼€å‘å·¥å…·ç¼“å­˜ (.pytest_cache, .mypy_cache ç­‰)")
        print("  3. è¿è¡Œæ—¶ç¼“å­˜ (CacheManager)")
        print("  4. æ—¥å¿—æ–‡ä»¶")
        print()
    else:
        print("ğŸ“‹ æ¸…ç†é¡¹ç›®:")
        print("  1. SQLite æ•°æ®åº“ï¼ˆçŸ¥è¯†åº“ã€æ–‡æ¡£ã€åˆ†å—ï¼‰")
        print("  2. å‘é‡å­˜å‚¨ï¼ˆHNSW ç´¢å¼•å’Œå…ƒæ•°æ®ï¼‰")
        print("  3. ä¸´æ—¶ä¸Šä¼ æ–‡ä»¶")
        print("  4. å¤„ç†åçš„åˆ†å—")
        print("  5. Python ç¼–è¯‘ç¼“å­˜ (__pycache__, .pyc)")
        print("  6. å¼€å‘å·¥å…·ç¼“å­˜ (.pytest_cache, .mypy_cache ç­‰)")
        print("  7. è¿è¡Œæ—¶ç¼“å­˜ (CacheManager)")
        print("  8. æ—¥å¿—æ–‡ä»¶")
        print()

    # ç¡®è®¤
    if args.all or args.only_cache:
        mode = "ç¼“å­˜" if args.only_cache else "æ‰€æœ‰æ•°æ®"
        should_clear = confirm(f"âš ï¸  ç¡®è®¤æ¸…é™¤{mode}ï¼Ÿ")
    else:
        should_clear = confirm("âš ï¸  ç¡®è®¤æ¸…é™¤ä»¥ä¸Šæ‰€æœ‰æ•°æ®ï¼Ÿ")

    if not should_clear:
        print("\nâŒ æ“ä½œå·²å–æ¶ˆ")
        logger.warning("ç”¨æˆ·å–æ¶ˆæ“ä½œ")
        return

    # å¤‡ä»½ç¡®è®¤
    backup_file = None
    if not args.only_cache and not args.no_backup and SQL_DB_FILE.exists():
        if args.backup:
            should_backup = True
        else:
            should_backup = confirm("\nğŸ’¾ æ˜¯å¦åœ¨æ¸…ç†å‰å¤‡ä»½æ•°æ®åº“ï¼Ÿ", default=True)

        if should_backup:
            print("\nå¤‡ä»½æ•°æ®åº“ä¸­...")
            backup_file = backup_database(SQL_DB_FILE)
            if backup_file:
                print(f"âœ“ å¤‡ä»½å®Œæˆ: {backup_file}\n")
    elif args.no_backup:
        print("\nâš ï¸  å·²ç¦ç”¨å¤‡ä»½åŠŸèƒ½")

    # æ‰§è¡Œæ¸…ç†
    print("å¼€å§‹æ¸…ç†...")
    print("-" * 70)

    success_count = 0
    total_count = 0

    if not args.only_cache:
        # æ•°æ®æ¸…ç†
        data_operations = [
            ("æ•°æ®åº“", lambda: clear_database(SQL_DB_FILE, stats)),
            ("å‘é‡å­˜å‚¨", lambda: clear_directory(VECTOR_STORE_DIR, stats, "å‘é‡å­˜å‚¨")),
            ("ä¸´æ—¶ä¸Šä¼ æ–‡ä»¶", lambda: clear_directory(TEMP_UPLOADS_DIR, stats, "ä¸´æ—¶ä¸Šä¼ æ–‡ä»¶")),
            ("å¤„ç†åçš„åˆ†å—", lambda: clear_directory(PROCESSED_CHUNKS_DIR, stats, "å¤„ç†åçš„åˆ†å—")),
        ]

        for name, func in data_operations:
            total_count += 1
            try:
                if func():
                    success_count += 1
            except Exception as e:
                logger.error(f"âœ— {name}æ¸…ç†å¤±è´¥: {e}")
                stats.add_error(f"{name}æ¸…ç†å¤±è´¥: {e}")

    # ç¼“å­˜æ¸…ç†ï¼ˆå§‹ç»ˆæ‰§è¡Œï¼‰
    cache_operations = [
        ("Python ç¼–è¯‘ç¼“å­˜", lambda: clear_pycache(stats)),
        ("å¼€å‘å·¥å…·ç¼“å­˜", lambda: clear_dev_caches(stats)),
        ("è¿è¡Œæ—¶ç¼“å­˜", clear_runtime_cache),
        ("æ—¥å¿—æ–‡ä»¶", lambda: clear_logs(LOGS_DIR, stats)),
    ]

    for name, func in cache_operations:
        total_count += 1
        try:
            if func():
                success_count += 1
        except Exception as e:
            logger.error(f"âœ— {name}æ¸…ç†å¤±è´¥: {e}")
            stats.add_error(f"{name}æ¸…ç†å¤±è´¥: {e}")

    # æ€»ç»“
    print("\n" + "=" * 70)
    if success_count == total_count:
        print("âœ“ æ¸…ç†å®Œæˆï¼æ‰€æœ‰æ•°æ®å·²æˆåŠŸæ¸…é™¤")
    else:
        print(f"âš ï¸  æ¸…ç†å®Œæˆï¼Œä½†æœ‰ {total_count - success_count} é¡¹æ“ä½œå¤±è´¥")

    print(f"\nğŸ“Š æ¸…ç†ç»Ÿè®¡: {stats.summary()}")

    if stats.errors:
        print(f"\nâŒ é”™è¯¯åˆ—è¡¨ ({len(stats.errors)} ä¸ª):")
        for error in stats.errors[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ªé”™è¯¯
            print(f"   â€¢ {error}")
        if len(stats.errors) > 5:
            print(f"   ... è¿˜æœ‰ {len(stats.errors) - 5} ä¸ªé”™è¯¯")

    print(f"\nç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    if backup_file:
        print(f"\nğŸ’¾ å¤‡ä»½æ–‡ä»¶ä½ç½®: {backup_file}")
        print(f"   å¦‚éœ€æ¢å¤ï¼Œè¯·æ‰‹åŠ¨å¤åˆ¶å¤‡ä»½æ–‡ä»¶åˆ° {SQL_DB_FILE}")

    print("=" * 70 + "\n")

    logger.info("=" * 70)
    logger.info(f"æ•°æ®æ¸…ç†å®Œæˆ: {success_count}/{total_count} é¡¹æ“ä½œæˆåŠŸ")
    logger.info(f"æ¸…ç†ç»Ÿè®¡: {stats.summary()}")
    logger.info("=" * 70)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        logger.warning("ç”¨æˆ·ä¸­æ–­æ¸…ç†æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        print(f"\nâœ— æ¸…ç†è„šæœ¬å‡ºé”™: {e}")
        logger.error(f"æ¸…ç†è„šæœ¬å‡ºé”™: {e}", exc_info=True)
        sys.exit(1)
