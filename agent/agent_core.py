"""
Agent Ê†∏ÂøÉÈÄªËæë - ÂçèË∞É LLM„ÄÅEmbedding ÂíåÊ£ÄÁ¥¢Ê®°Âùó
ÊîØÊåÅÂ§öÂ±ÇÁºìÂ≠òÔºöÊü•ËØ¢ÁªìÊûúÁºìÂ≠ò„ÄÅÊ£ÄÁ¥¢ÂàÜÁ±ªÁºìÂ≠ò„ÄÅEmbedding ÁºìÂ≠ò„ÄÅËØ≠‰πâÂåñÁºìÂ≠ò
"""

from typing import Optional, Dict, Any, AsyncGenerator, List
from datetime import datetime
import uuid
import logging
from .state import AgentState
from .graph import create_agent_graph
from config.settings import settings
import time
from utils.cache_manager import get_cache_manager
from utils.performance_tracker import PerformanceTracker

logger = logging.getLogger(__name__)


class AgentCore:
    """
    Agent Ê†∏ÂøÉÁ±ª
    ÂçèË∞É LLM„ÄÅEmbedding„ÄÅÊ£ÄÁ¥¢Á≠âÊ®°ÂùóÔºåÊèê‰æõÁªü‰∏ÄÁöÑ Agent Êé•Âè£
    ÊîØÊåÅÂ§öÂ±ÇÁºìÂ≠ò‰ºòÂåñÊü•ËØ¢ÊÄßËÉΩ
    """

    def __init__(self, enable_cache: bool = True):
        """
        ÂàùÂßãÂåñ Agent Ê†∏ÂøÉ

        Args:
            enable_cache: ÊòØÂê¶ÂêØÁî®ÁºìÂ≠òÔºàÂåÖÊã¨ËØ≠‰πâÂåñÁºìÂ≠òÔºâ
        """
        self.agent_graph = create_agent_graph()
        self.enable_cache = enable_cache
        self.cache_manager = get_cache_manager() if enable_cache else None

        logger.info(
            f"Agent Ê†∏ÂøÉÂ∑≤ÂàùÂßãÂåñ (cache={'enabled' if enable_cache else 'disabled'})"
        )

    def _create_initial_state(
        self,
        kb_id: str,
        question: str,
        top_k: int = 5,
    ) -> AgentState:
        """
        ÂàõÂª∫ÂàùÂßãÁä∂ÊÄÅ

        Args:
            kb_id: Áü•ËØÜÂ∫ì ID
            question: Áî®Êà∑ÈóÆÈ¢ò
            top_k: Ê£ÄÁ¥¢ÁªìÊûúÊï∞Èáè

        Returns:
            ÂàùÂßã AgentState
        """
        query_id = str(uuid.uuid4())
        state = AgentState(
            query_id=query_id,
            kb_id=kb_id,
            question=question,
            max_iterations=settings.LANGGRAPH_MAX_ITERATIONS,
        )
        state.add_message("user", question)
        return state

    async def execute_query(
        self,
        kb_id: str,
        question: str,
        top_k: int = 5,
        use_tools: bool = False,
        use_cache: bool = True,
    ) -> Dict[str, Any]:
        """
        ÊâßË°åÊü•ËØ¢ÔºàÊîØÊåÅÂ§öÂ±ÇÁºìÂ≠òÂíåÊÄßËÉΩËøΩË∏™Ôºâ

        Args:
            kb_id: Áü•ËØÜÂ∫ì ID
            question: Áî®Êà∑ÈóÆÈ¢ò
            top_k: Ê£ÄÁ¥¢ÁªìÊûúÊï∞Èáè
            use_tools: ÊòØÂê¶‰ΩøÁî®Â∑•ÂÖ∑
            use_cache: ÊòØÂê¶‰ΩøÁî®ÁºìÂ≠ò

        Returns:
            Êü•ËØ¢ÁªìÊûú
        """
        query_id = str(uuid.uuid4())
        logger.info(f"[{query_id}] üöÄ ÂºÄÂßãÊâßË°åÊü•ËØ¢: kb_id={kb_id}, question={question}")
        print(f"\nüìã Êèê‰∫§ÈóÆÈ¢ò: {question}")

        start_time = time.time()

        try:
            # Ê£ÄÊü•Êü•ËØ¢ÁªìÊûúÁºìÂ≠òÔºàÊîØÊåÅËØ≠‰πâÂåπÈÖçÔºâ
            if use_cache and self.enable_cache and self.cache_manager:
                try:
                    cached_result = self.cache_manager.query_cache.get_result(kb_id, question)
                    if cached_result is not None:
                        response_time_ms = (time.time() - start_time) * 1000
                        cached_result["response_time_ms"] = response_time_ms
                        cached_result["from_cache"] = True
                        logger.info(f"[{query_id}] ‚ö° ÁºìÂ≠òÂëΩ‰∏≠ ({response_time_ms:.2f}ms)")
                        print(f"‚ö° ÁºìÂ≠òÂëΩ‰∏≠ ({response_time_ms:.0f}ms)")
                        return cached_result

                except Exception as e:
                    logger.warning(f"[{query_id}] ÁºìÂ≠òÊü•ËØ¢Â§±Ë¥•ÔºàÁªßÁª≠ÊâßË°åÔºâ: {e}")

            # ÂàõÂª∫ÊÄßËÉΩËøΩË∏™Âô®
            tracker = PerformanceTracker(query_id)

            # ÂàõÂª∫ÂàùÂßãÁä∂ÊÄÅ
            initial_state = self._create_initial_state(kb_id, question, top_k)
            initial_state.query_id = query_id  # Êõ¥Êñ∞ query_id

            # ÊâßË°å Agent Âõæ
            final_state = await self.agent_graph.execute(initial_state)

            # ËÆ°ÁÆóÂìçÂ∫îÊó∂Èó¥
            response_time_ms = (time.time() - start_time) * 1000

            # Ê†ºÂºèÂåñÂìçÂ∫î
            response = {
                "query_id": query_id,
                "kb_id": initial_state.kb_id,
                "question": initial_state.question,
                "answer": "Êó†Ê≥ïÁîüÊàêÁ≠îÊ°à",
                "retrieved_docs": [],
                "sources": [],
                "confidence": 0.0,
                "response_time_ms": response_time_ms,
                "from_cache": False,
                "metadata": {
                    "iterations": 0,
                    "intermediate_steps": [],
                    "error": None,
                },
            }

            # Â∞ùËØï‰ªé final_state ‰∏≠ÊèêÂèñÂÖ∂‰ªñ‰ø°ÊÅØ
            try:
                # Â§ÑÁêÜ‰∏§ÁßçÊÉÖÂÜµÔºöfinal_state ÊòØÂ≠óÂÖ∏Êàñ AgentState ÂØπË±°
                if isinstance(final_state, dict):
                    # Â¶ÇÊûú final_state ÊòØÂ≠óÂÖ∏

                    response["query_id"] = final_state.get("query_id", query_id)
                    response["kb_id"] = final_state.get("kb_id", initial_state.kb_id)
                    response["question"] = final_state.get("question", initial_state.question)
                    response["answer"] = final_state.get("answer", "Êó†Ê≥ïÁîüÊàêÁ≠îÊ°à")

                    # Â§ÑÁêÜ retrieved_docs
                    retrieved_docs = final_state.get("retrieved_docs", [])
                    if retrieved_docs:
                        docs_list = []
                        for doc in retrieved_docs:
                            if hasattr(doc, 'id'):
                                # Â¶ÇÊûúÊòØÂØπË±°
                                docs_list.append({
                                    "id": doc.id,
                                    "content": doc.content,
                                    "score": doc.score,
                                    "metadata": doc.metadata,
                                })
                            elif isinstance(doc, dict):
                                # Â¶ÇÊûúÊòØÂ≠óÂÖ∏
                                docs_list.append(doc.copy())

                        response["retrieved_docs"] = docs_list

                    response["sources"] = final_state.get("sources", [])
                    response["confidence"] = final_state.get("confidence", 0.0)

                    # Â§ÑÁêÜ metadata
                    response["metadata"]["iterations"] = final_state.get("iteration", 0)
                    response["metadata"]["intermediate_steps"] = final_state.get("intermediate_steps", [])
                    response["metadata"]["error"] = final_state.get("error", None)

                elif hasattr(final_state, '__dict__'):

                    if hasattr(final_state, 'answer') and final_state.answer:
                        response["answer"] = final_state.answer
                    if hasattr(final_state, 'retrieved_docs'):
                        response["retrieved_docs"] = [
                            {
                                "id": doc.id,
                                "content": doc.content,
                                "score": doc.score,
                                "metadata": doc.metadata,
                            }
                            for doc in final_state.retrieved_docs
                        ]
                    if hasattr(final_state, 'sources'):
                        response["sources"] = final_state.sources
                    if hasattr(final_state, 'confidence'):
                        response["confidence"] = final_state.confidence
                    if hasattr(final_state, 'iteration'):
                        response["metadata"]["iterations"] = final_state.iteration
                    if hasattr(final_state, 'intermediate_steps'):
                        response["metadata"]["intermediate_steps"] = final_state.intermediate_steps
                    if hasattr(final_state, 'error'):
                        response["metadata"]["error"] = final_state.error

            except Exception as e:
                logger.error(f"‰ªé final_state ÊèêÂèñ‰ø°ÊÅØÂ§±Ë¥•: {e}")

            # ÁºìÂ≠òÊü•ËØ¢ÁªìÊûúÔºàÊîØÊåÅËØ≠‰πâÂåπÈÖçÔºâ
            if use_cache and self.enable_cache and self.cache_manager:
                try:
                    cache_result = response.copy()
                    # ‰øùÂ≠òÂà∞ÁºìÂ≠òÔºàËá™Âä®ÊîØÊåÅËØ≠‰πâÂåπÈÖçÔºâ
                    self.cache_manager.query_cache.set_result(kb_id, question, cache_result)
                except Exception as e:
                    logger.warning(f"ÁºìÂ≠ò‰øùÂ≠òÂ§±Ë¥•Ôºà‰∏çÂΩ±Âìç‰∏ªÊµÅÁ®ãÔºâ: {e}")

            logger.info(f"[{query_id}] ‚úÖ Êü•ËØ¢ÂÆåÊàê, ÊÄªËÄóÊó∂ {response_time_ms:.2f}ms")
            print(f"‚úÖ ÂÆåÊàê ({response_time_ms:.0f}ms)")

            return response

        except Exception as e:
            elapsed = (time.time() - start_time) * 1000
            logger.error(f"[{query_id}] ‚ùå Êü•ËØ¢ÊâßË°åÂ§±Ë¥• ({elapsed:.2f}ms): {e}", exc_info=True)
            print(f"‚ùå ÈîôËØØ: {e}")
            return {
                "query_id": query_id,
                "kb_id": kb_id,
                "question": question,
                "answer": f"Êü•ËØ¢ÊâßË°åÂ§±Ë¥•: {e}",
                "error": str(e),
                "response_time_ms": elapsed,
                "from_cache": False,
            }

    async def stream_query(
        self,
        kb_id: str,
        question: str,
        top_k: int = 5,
        use_tools: bool = False,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        ÊµÅÂºèÊâßË°åÊü•ËØ¢

        Args:
            kb_id: Áü•ËØÜÂ∫ì ID
            question: Áî®Êà∑ÈóÆÈ¢ò
            top_k: Ê£ÄÁ¥¢ÁªìÊûúÊï∞Èáè
            use_tools: ÊòØÂê¶‰ΩøÁî®Â∑•ÂÖ∑

        Yields:
            ÊµÅÂºèÁªìÊûú
        """
        logger.info(
            f"ÂºÄÂßãÊµÅÂºèÊü•ËØ¢: kb_id={kb_id}, question={question}"
        )

        start_time = time.time()

        try:
            # ÂàõÂª∫ÂàùÂßãÁä∂ÊÄÅ
            initial_state = self._create_initial_state(kb_id, question, top_k)

            # ÊµÅÂºèÊâßË°å Agent Âõæ
            async for event in self.agent_graph.stream(initial_state):
                response_time_ms = (time.time() - start_time) * 1000
                yield {
                    "query_id": initial_state.query_id,
                    "event": event,
                    "response_time_ms": response_time_ms,
                }

            logger.info(f"ÊµÅÂºèÊü•ËØ¢ÂÆåÊàê: query_id={initial_state.query_id}")

        except Exception as e:
            logger.error(f"ÊµÅÂºèÊü•ËØ¢ÊâßË°åÂ§±Ë¥•: {e}")
            yield {
                "error": str(e),
                "response_time_ms": (time.time() - start_time) * 1000,
            }

    def get_agent_info(self) -> Dict[str, Any]:
        """
        Ëé∑Âèñ Agent ‰ø°ÊÅØ

        Returns:
            Agent ÈÖçÁΩÆ‰ø°ÊÅØ
        """
        return {
            "project": settings.PROJECT_NAME,
            "version": settings.PROJECT_VERSION,
            "llm_model": settings.LLM_MODEL_NAME,
            "embedding_model": settings.EMBEDDING_MODEL_NAME,
            "vector_store": settings.VECTOR_STORE_TYPE,
            "chunk_size": settings.TEXT_CHUNK_SIZE,
            "max_iterations": settings.LANGGRAPH_MAX_ITERATIONS,
            "timeout": settings.LANGGRAPH_TIMEOUT,
        }
