"""
å®Œæ•´æ•°æ®åº“æ¸…ç†è„šæœ¬ - æ¸…é™¤å‘é‡æ•°æ®åº“ + SQLite æ•°æ®åº“ä¸­çš„æ‰€æœ‰æ–‡æ¡£å’ŒçŸ¥è¯†åº“æ•°æ®
"""

import os
import sqlite3
import shutil
import logging
from pathlib import Path
from datetime import datetime
from config.settings import settings
from retrieval.vector_store_client import VectorStoreClient

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


class DatabaseCleaner:
    """æ•°æ®åº“æ¸…ç†å·¥å…·"""

    def __init__(self):
        """åˆå§‹åŒ–æ¸…ç†å·¥å…·"""
        self.db_path = str(settings.PROJECT_ROOT / settings.DATABASE_URL.replace("sqlite:///./", ""))
        self.vector_db_path = settings.VECTOR_STORE_PATH
        self.temp_upload_path = settings.TEMP_UPLOAD_PATH
        self.processed_chunks_path = settings.PROCESSED_CHUNKS_PATH

        logger.info("æ•°æ®åº“æ¸…ç†å·¥å…·å·²åˆå§‹åŒ–")
        logger.info(f"SQLite æ•°æ®åº“: {self.db_path}")
        logger.info(f"å‘é‡æ•°æ®åº“: {self.vector_db_path}")

    def backup_database(self):
        """å¤‡ä»½æ•°æ®åº“"""
        logger.info("\n[æ­¥éª¤1] å¤‡ä»½æ•°æ®åº“...")

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_dir = Path(self.db_path).parent / "backup"
        backup_dir.mkdir(exist_ok=True)

        # å¤‡ä»½ SQLite æ•°æ®åº“
        if os.path.exists(self.db_path):
            backup_db_path = backup_dir / f"kbrobot_{timestamp}.db"
            shutil.copy2(self.db_path, backup_db_path)
            logger.info(f"âœ“ SQLite æ•°æ®åº“å·²å¤‡ä»½: {backup_db_path}")

        # å¤‡ä»½å‘é‡æ•°æ®åº“ç›®å½•
        if os.path.exists(self.vector_db_path):
            backup_vector_path = backup_dir / f"vector_store_{timestamp}"
            shutil.copytree(self.vector_db_path, backup_vector_path)
            logger.info(f"âœ“ å‘é‡æ•°æ®åº“å·²å¤‡ä»½: {backup_vector_path}")

        logger.info(f"âœ“ æ‰€æœ‰æ•°æ®å·²å¤‡ä»½åˆ°: {backup_dir}")
        return backup_dir

    def clean_sqlite_database(self):
        """æ¸…é™¤ SQLite æ•°æ®åº“ä¸­çš„æ‰€æœ‰æ•°æ®"""
        logger.info("\n[æ­¥éª¤2] æ¸…é™¤ SQLite æ•°æ®åº“æ•°æ®...")

        if not os.path.exists(self.db_path):
            logger.warning(f"âš ï¸  SQLite æ•°æ®åº“ä¸å­˜åœ¨: {self.db_path}")
            return 0, 0, 0

        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        try:
            conn.execute("BEGIN TRANSACTION")

            # æ¸…é™¤æ‰€æœ‰æ•°æ®ï¼Œä½†ä¿ç•™è¡¨ç»“æ„
            cursor.execute("DELETE FROM text_chunks")
            chunks_deleted = cursor.rowcount
            logger.info(f"  âœ“ å·²åˆ é™¤ {chunks_deleted} ä¸ªæ–‡æœ¬åˆ†å—")

            cursor.execute("DELETE FROM documents")
            docs_deleted = cursor.rowcount
            logger.info(f"  âœ“ å·²åˆ é™¤ {docs_deleted} ä¸ªæ–‡æ¡£")

            cursor.execute("DELETE FROM knowledge_bases")
            kbs_deleted = cursor.rowcount
            logger.info(f"  âœ“ å·²åˆ é™¤ {kbs_deleted} ä¸ªçŸ¥è¯†åº“")

            conn.commit()
            logger.info("  âœ“ SQLite æ•°æ®åº“å·²æ¸…ç©º")

            return chunks_deleted, docs_deleted, kbs_deleted

        except Exception as e:
            conn.rollback()
            logger.error(f"  âŒ SQLite æ•°æ®åº“æ¸…ç©ºå¤±è´¥: {e}")
            raise
        finally:
            conn.close()

    def clean_vector_database(self):
        """æ¸…é™¤å‘é‡æ•°æ®åº“"""
        logger.info("\n[æ­¥éª¤3] æ¸…é™¤å‘é‡æ•°æ®åº“...")

        try:
            # åˆå§‹åŒ–å‘é‡å­˜å‚¨å®¢æˆ·ç«¯
            vector_store = VectorStoreClient(
                store_type=settings.VECTOR_STORE_TYPE,
                path_or_url=self.vector_db_path,
                collection_name=settings.VECTOR_STORE_COLLECTION_NAME,
            )

            # å°è¯•æ¸…ç©ºé›†åˆ
            try:
                vector_store.clear_collection()
                logger.info(f"  âœ“ é›†åˆå·²æ¸…ç©º: {settings.VECTOR_STORE_COLLECTION_NAME}")
            except Exception as e:
                logger.info(f"  â„¹ï¸  é›†åˆæ¸…ç©ºå¼‚å¸¸ï¼ˆå¯èƒ½ä¸å­˜åœ¨ï¼‰: {e}")

            # åˆ é™¤æ•´ä¸ªå‘é‡æ•°æ®åº“ç›®å½•
            if os.path.exists(self.vector_db_path):
                logger.info(f"  åˆ é™¤å‘é‡æ•°æ®åº“ç›®å½•: {self.vector_db_path}")
                shutil.rmtree(self.vector_db_path)
                logger.info(f"  âœ“ å‘é‡æ•°æ®åº“ç›®å½•å·²åˆ é™¤")

                # é‡æ–°åˆ›å»ºç©ºç›®å½•
                os.makedirs(self.vector_db_path, exist_ok=True)
                logger.info(f"  âœ“ å‘é‡æ•°æ®åº“ç›®å½•å·²é‡æ–°åˆ›å»º")
            else:
                logger.info(f"  â„¹ï¸  å‘é‡æ•°æ®åº“ç›®å½•ä¸å­˜åœ¨")

        except Exception as e:
            logger.error(f"  âŒ å‘é‡æ•°æ®åº“æ¸…é™¤å¤±è´¥: {e}")

    def clean_temp_files(self):
        """æ¸…é™¤ä¸´æ—¶æ–‡ä»¶"""
        logger.info("\n[æ­¥éª¤4] æ¸…é™¤ä¸´æ—¶æ–‡ä»¶...")

        deleted_count = 0
        if os.path.exists(self.temp_upload_path):
            for filename in os.listdir(self.temp_upload_path):
                file_path = os.path.join(self.temp_upload_path, filename)
                if os.path.isfile(file_path):
                    try:
                        os.remove(file_path)
                        deleted_count += 1
                    except Exception as e:
                        logger.warning(f"  âš ï¸  åˆ é™¤å¤±è´¥: {filename}, é”™è¯¯: {e}")

            logger.info(f"  âœ“ å·²åˆ é™¤ {deleted_count} ä¸ªä¸´æ—¶æ–‡ä»¶")
        else:
            logger.info(f"  â„¹ï¸  ä¸´æ—¶æ–‡ä»¶ç›®å½•ä¸å­˜åœ¨")

        return deleted_count

    def clean_chunk_files(self):
        """æ¸…é™¤åˆ†å—æ–‡ä»¶"""
        logger.info("\n[æ­¥éª¤5] æ¸…é™¤åˆ†å—æ–‡ä»¶...")

        deleted_count = 0
        if os.path.exists(self.processed_chunks_path):
            for filename in os.listdir(self.processed_chunks_path):
                file_path = os.path.join(self.processed_chunks_path, filename)
                if os.path.isfile(file_path):
                    try:
                        os.remove(file_path)
                        deleted_count += 1
                    except Exception as e:
                        logger.warning(f"  âš ï¸  åˆ é™¤å¤±è´¥: {filename}, é”™è¯¯: {e}")

            logger.info(f"  âœ“ å·²åˆ é™¤ {deleted_count} ä¸ªåˆ†å—æ–‡ä»¶")
        else:
            logger.info(f"  â„¹ï¸  åˆ†å—æ–‡ä»¶ç›®å½•ä¸å­˜åœ¨")

        return deleted_count

    def verify_cleanup(self):
        """éªŒè¯æ¸…ç†ç»“æœ"""
        logger.info("\n[æ­¥éª¤6] éªŒè¯æ¸…ç†ç»“æœ...")

        # éªŒè¯ SQLite æ•°æ®åº“
        logger.info("\n  ğŸ“Š SQLite æ•°æ®åº“ç»Ÿè®¡:")
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute("SELECT COUNT(*) FROM knowledge_bases")
        kb_count = cursor.fetchone()[0]
        logger.info(f"    - çŸ¥è¯†åº“æ•°: {kb_count}")

        cursor.execute("SELECT COUNT(*) FROM documents")
        doc_count = cursor.fetchone()[0]
        logger.info(f"    - æ–‡æ¡£æ•°: {doc_count}")

        cursor.execute("SELECT COUNT(*) FROM text_chunks")
        chunk_count = cursor.fetchone()[0]
        logger.info(f"    - åˆ†å—æ•°: {chunk_count}")

        conn.close()

        # éªŒè¯å‘é‡æ•°æ®åº“
        logger.info("\n  ğŸ“Š å‘é‡æ•°æ®åº“ç»Ÿè®¡:")
        try:
            vector_store = VectorStoreClient(
                store_type=settings.VECTOR_STORE_TYPE,
                path_or_url=self.vector_db_path,
                collection_name=settings.VECTOR_STORE_COLLECTION_NAME,
            )
            stats = vector_store.get_collection_stats()
            logger.info(f"    - é›†åˆ: {stats.get('collection_name')}")
            logger.info(f"    - å‘é‡æ•°: {stats.get('count', 0)}")
        except Exception as e:
            logger.info(f"    - é›†åˆ: ç©º (å¼‚å¸¸: {e})")

        # éªŒè¯æ–‡ä»¶ç³»ç»Ÿ
        logger.info("\n  ğŸ“Š æ–‡ä»¶ç³»ç»Ÿç»Ÿè®¡:")
        temp_count = len(os.listdir(self.temp_upload_path)) if os.path.exists(self.temp_upload_path) else 0
        chunk_count_fs = len(os.listdir(self.processed_chunks_path)) if os.path.exists(self.processed_chunks_path) else 0
        logger.info(f"    - ä¸´æ—¶æ–‡ä»¶: {temp_count}")
        logger.info(f"    - åˆ†å—æ–‡ä»¶: {chunk_count_fs}")

        # æ€»ä½“åˆ¤æ–­
        is_clean = kb_count == 0 and doc_count == 0 and chunk_count == 0 and temp_count == 0 and chunk_count_fs == 0

        if is_clean:
            logger.info("\n  âœ… æ•°æ®åº“å·²å®Œå…¨æ¸…ç©ºï¼")
        else:
            logger.warning("\n  âš ï¸  æ•°æ®åº“ä¸­ä»æœ‰æ®‹ç•™æ•°æ®")

        return is_clean

    def full_cleanup(self, backup=True):
        """æ‰§è¡Œå®Œæ•´æ¸…ç†"""
        logger.info("="*60)
        logger.info("å¼€å§‹æ‰§è¡Œå®Œæ•´æ•°æ®åº“æ¸…ç†")
        logger.info("="*60)

        try:
            # å¤‡ä»½
            if backup:
                backup_dir = self.backup_database()

            # æ¸…ç†å„ä¸ªéƒ¨åˆ†
            chunks_del, docs_del, kbs_del = self.clean_sqlite_database()
            self.clean_vector_database()
            temp_files_del = self.clean_temp_files()
            chunk_files_del = self.clean_chunk_files()

            # éªŒè¯
            is_clean = self.verify_cleanup()

            # æ€»ç»“
            logger.info("\n" + "="*60)
            logger.info("æ•°æ®åº“æ¸…ç†å®Œæˆ")
            logger.info("="*60)
            logger.info("\nğŸ“Š æ¸…ç†æ‘˜è¦:")
            logger.info(f"  - SQLite æ•°æ®åº“:")
            logger.info(f"    â€¢ åˆ é™¤çŸ¥è¯†åº“: {kbs_del} ä¸ª")
            logger.info(f"    â€¢ åˆ é™¤æ–‡æ¡£: {docs_del} ä¸ª")
            logger.info(f"    â€¢ åˆ é™¤åˆ†å—: {chunks_del} ä¸ª")
            logger.info(f"  - å‘é‡æ•°æ®åº“: å·²æ¸…ç©º")
            logger.info(f"  - ä¸´æ—¶æ–‡ä»¶: {temp_files_del} ä¸ª")
            logger.info(f"  - åˆ†å—æ–‡ä»¶: {chunk_files_del} ä¸ª")

            if backup:
                logger.info(f"  - å¤‡ä»½ä½ç½®: {backup_dir}")

            if is_clean:
                logger.info("\nâœ… æ‰€æœ‰æ•°æ®å·²å½»åº•æ¸…é™¤ï¼")
            else:
                logger.warning("\nâš ï¸  æ¸…ç†å®Œæˆï¼Œä½†ä»æœ‰æ®‹ç•™æ•°æ®")

            return True

        except Exception as e:
            logger.error(f"âŒ æ¸…ç†å¤±è´¥: {e}", exc_info=True)
            return False


def main():
    """ä¸»å‡½æ•°"""
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "--help":
        print("ç”¨æ³•:")
        print("  python clear_all_db.py          # æ¸…é™¤æ‰€æœ‰æ•°æ®åº“ï¼ˆåŒ…å«å¤‡ä»½ï¼‰")
        print("  python clear_all_db.py --no-backup # æ¸…é™¤æ‰€æœ‰æ•°æ®åº“ï¼ˆä¸å¤‡ä»½ï¼‰")
        print("  python clear_all_db.py --help   # æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯")
        sys.exit(0)

    # äºŒæ¬¡ç¡®è®¤
    print("\n" + "="*60)
    print("âš ï¸  ä¸¥é‡è­¦å‘Šï¼šæ‚¨å³å°†æ¸…é™¤ALLæ•°æ®åº“ï¼")
    print("="*60)
    print("\nè¿™å°†æ°¸ä¹…åˆ é™¤:")
    print("  âœ— SQLite æ•°æ®åº“ä¸­çš„æ‰€æœ‰çŸ¥è¯†åº“")
    print("  âœ— SQLite æ•°æ®åº“ä¸­çš„æ‰€æœ‰æ–‡æ¡£")
    print("  âœ— SQLite æ•°æ®åº“ä¸­çš„æ‰€æœ‰æ–‡æœ¬åˆ†å—")
    print("  âœ— HNSW å‘é‡æ•°æ®åº“ä¸­çš„æ‰€æœ‰å‘é‡")
    print("  âœ— data/temp_uploads ä¸­çš„æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶")
    print("  âœ— data/processed_chunks ä¸­çš„æ‰€æœ‰åˆ†å—æ–‡ä»¶")
    print("\næ­¤æ“ä½œæ— æ³•æ’¤é”€ï¼")
    print("="*60)

    # è·å–ç”¨æˆ·ç¡®è®¤
    response = input("\næ‚¨ç¡®å®šè¦ç»§ç»­å—ï¼Ÿ(è¯·è¾“å…¥ 'yes' ç¡®è®¤): ").strip().lower()

    if response == "yes":
        backup = "--no-backup" not in sys.argv
        print("\næ­£åœ¨æ¸…é™¤æ•°æ®åº“...\n")

        cleaner = DatabaseCleaner()
        success = cleaner.full_cleanup(backup=backup)

        if success:
            print("\nâœ… æ¸…ç†å®Œæˆï¼")
        else:
            print("\nâŒ æ¸…ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯")

    else:
        print("\nâŒ æ“ä½œå·²å–æ¶ˆ")


if __name__ == "__main__":
    main()
